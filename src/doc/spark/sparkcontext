sparkcontext
action 行动算子
	val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)

sparkcontext 的runjob
启动作业 作业切分
  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
  runJob 作业切分
  val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
提交作业
   val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)


   eventProcessLoop.post(JobSubmitted(
      jobId, rdd, func2, partitions.toArray, callSite, waiter,
      SerializationUtils.clone(properties)))
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop#doOnReceive
       case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>
      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)

 处理jobJobSubmitted
       dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)
创建最终阶段ResultStage
 var finalStage: ResultStage = null 
finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)
	根据是否shuffle 判断是否有父阶段 创建mapshufflestage 
		val parents = getOrCreateParentStages(rdd, jobId)
	提交最终阶段 （递归调用提交stage 从后往前）
	 submitStage(finalStage)

	 private def submitStage(stage: Stage) {
    val jobId = activeJobForStage(stage)
    if (jobId.isDefined) {
      logDebug("submitStage(" + stage + ")")
      if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {
        val missing = getMissingParentStages(stage).sortBy(_.id)
        logDebug("missing: " + missing)
        if (missing.isEmpty) {
          logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
          submitMissingTasks(stage, jobId.get) // 提交任务task 
        } else {
          for (parent <- missing) { // 如果有父stage 则提交父stage
            submitStage(parent)
          }
          waitingStages += stage
        }
      }
    } else {
      abortStage(stage, "No active job for stage " + stage.id, None)
    }
  }

提交tesk
submitMissingTasks(stage, jobId.get) // 提交任务task 
   stage match { // 为stage 做模式匹配
        case stage: ShuffleMapStage => // ShuffleMapStage
          stage.pendingPartitions.clear()
          partitionsToCompute.map { id =>
            val locs = taskIdToLocations(id)
            val part = partitions(id)
            stage.pendingPartitions += id
            new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,// 遍历每个分区，为每个分区创建一个ShuffleMapTask
              taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),
              Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())
          }

        case stage: ResultStage => // ResultStage 为
          partitionsToCompute.map { id => // 遍历每个分区，为每个分区创建一个task
            val p: Int = stage.partitions(id)
            val part = partitions(p)
            val locs = taskIdToLocations(id)
            new ResultTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, id, properties, serializedTaskMetrics,
              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,
              stage.rdd.isBarrier())
          }
      }

// 判断stage是否有task 有task 则由taskScheduler 提交task
 if (tasks.size > 0) {	// 判断stage是否有task 有task 则由taskScheduler 提交task
      logInfo(s"Submitting ${tasks.size} missing tasks from $stage (${stage.rdd}) (first 15 " +
        s"tasks are for partitions ${tasks.take(15).map(_.partitionId)})")
      taskScheduler.submitTasks(new TaskSet(
        tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties)) //提交当前stage 的所有分区
    }

    org.apache.spark.scheduler.TaskSchedulerImpl#submitTasks